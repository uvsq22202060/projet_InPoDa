{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    *** CE FICHIER SERT DE TESTS      (voir tout en bas)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################# PARTIE SUR LES IMPORTS  #############################################################\n",
    "import json                         # module pour load le file\n",
    "import re                           # module pour retourner les hasthtags\n",
    "from textblob import TextBlob       # module pour analyser le sentiment du tweet\n",
    "import pandas as pd                 # module pour visualiser et comparer les tweets\n",
    "import matplotlib.pyplot as plt     # module pour la création de Diagramme\n",
    "from datetime import datetime       # module pour manipuler l'heure \n",
    "import random as rd                 # module pour générer des éléments aléatoires\n",
    "import langid                       # module pour reconnaitre la langue d'un tweet\n",
    "import spacy                        # module pour extraire le(s) sujet(s) d'un tweet\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # chargement du \"Trained pipeline\" de spacy ici anglais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################# PARTIE SUR LA GESTION DES TWEETS #############################################################\n",
    "\n",
    "def text_cleaning(text):\n",
    "  '''retourne le texte entrée, nettoyer de tout symboles/emojis'''\n",
    "  # Pattern trouvé sur le lien suivant : https://gist.github.com/Alex-Just/e86110836f3f93fe7932290526529cd1#gistcomment-3208085\n",
    "  \n",
    "  pattern = re.compile(\n",
    "    \"([\"\n",
    "    \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "    \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "    \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "    \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "    \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "    \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    \"])\"\n",
    "                      )\n",
    "  \n",
    "  text = re.sub(pattern, r'', text)                              # ici l'expression regulière va supprimer (sub en anglais) les émojis et caractères contenus dans le pattern\n",
    "  return text\n",
    "\n",
    "def transfer(source, destination):\n",
    "  '''retourne un \"fichier_atterissage.json\", une base de donnée nettoyer'''\n",
    "  with open(source,\"r\") as source_file, open(destination,\"w\") as destination_file :       # on ouvre le fichier source (-> source_file) et fichier destination (-> destination_file)\n",
    "\n",
    "      content_source = source_file.read()                        # on extrait le contenu de notre fichier source que l'on attribue à la variable content_source\n",
    "      content_source = text_cleaning(content_source)             # on nettoie le contenu obtenu (pour supprimer les emojis...)\n",
    "\n",
    "      destination_file.write(content_source)                     # on écrit le contenu dans le fichier de destination\n",
    "\n",
    "#transfer(\"tweets.json\",\"fichier_atterrissage.json\")\n",
    "\n",
    "# Data utilisé pour la création des dictionnaires pour faciliter l'analyse de diagramme,...\n",
    "file = open(\"fichier_atterrissage.json\",\"r\")\n",
    "data =json.load(file)\n",
    "file.close()\n",
    "\n",
    "# Data utilisé pour analyser visuellement les différentes caractéristiques d'un tweet\n",
    "data2 = pd.read_json(\"fichier_atterrissage.json\")\n",
    "df_data = pd.DataFrame(data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################# PARTIE SUR LA CLASS \"Tweet\" #############################################################\n",
    "\n",
    "class Tweet :\n",
    "    def __init__(self,id_tweet,location_tweet,creation_tweet,retweet_count,tweet_language,tweet_text) :\n",
    "        self.id        = id_tweet\n",
    "        self.location  = location_tweet\n",
    "        self.created   = creation_tweet\n",
    "        self.retweet   = retweet_count\n",
    "        self.language  = tweet_language\n",
    "        self.text      = tweet_text\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Tweet id : {self.id}\\nAuthor Location : {self.location} \\nTweet Creation : {self.created} \\nNumber of Retweets : {self.retweet} \\nTweet Language : {self.language} \\nTweet Text : {self.text}\"\n",
    "    \n",
    "    def get_author(self):\n",
    "        id_tweet = self.id\n",
    "        return id_tweet\n",
    "    \n",
    "    def get_text(self):\n",
    "        return self.text\n",
    "    \n",
    "    def get_hashtags(self):\n",
    "        return re.findall(r\"#(\\w+)\", self.text)\n",
    "\n",
    "    def get_mention(self):\n",
    "        return re.findall(r\"@(\\w+)\", self.text)\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        text = TextBlob(self.text)\n",
    "        text_polarity = text.sentiment.polarity\n",
    "        if text_polarity > 0 :   return \"Positive\"\n",
    "        elif text_polarity < 0 : return \"Negative\"\n",
    "        else :                   return \"Neutral\"\n",
    "    \n",
    "    def get_topic(self):                                # https://spacy.io/usage/linguistic-features#pos-tagging\n",
    "        text = self.text\n",
    "        topics = []\n",
    "        doc = nlp(text)                                 # on analyse la phrase avec le module SpaCy\n",
    "        for token in doc :                              # on parcours chaquz élément de la phrase (token)\n",
    "            if token.text == \"#\": continue              # on esquive le \"#\" dans notre liste car ce n'est pas cohérent\n",
    "            elif token.pos_ == \"NOUN\" :                 # on vérifie si celui si est un Nom dans la phrase (sa position lexicale)\n",
    "                topics.append(token.text)               # dans ce cas là on l'ajoute à notre liste de sujets                                                        \n",
    "        return topics\n",
    "\n",
    "\n",
    "#instance / objets de la class \"Tweet\"\n",
    "tweets = [Tweet(tweet[\"id\"],tweet[\"AuthorLocation\"],tweet[\"CreatedAt\"],tweet[\"RetweetCount\"],tweet[\"TweetLanguage\"],tweet[\"TweetText\"]) for tweet in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "##########################################  AVEC LES DICTIONNAIRES   ####################################################\n",
    "#########################################################################################################################\n",
    "\n",
    "# Pour Top K Hashtags -> 2 fonctions\n",
    "\n",
    "# La première :\n",
    "def all_hashtag():\n",
    "    '''retourne tout les hashtags du fichier json dans une liste'''\n",
    "    l = []                                         # on intialise la liste qui va être retournée à l'éxécution de la fonction\n",
    "    for tweet in tweets:                           # on parcours tout les tweets (un par un)\n",
    "        if tweet.get_hashtags() == [] :             # on vérifie avec l'atribut de l'instance si la valeur retourner est  \"[]\" <- pas de valeur ; dans ce cas là on passe au tweet suivant\n",
    "            continue\n",
    "        else :\n",
    "            temp = tweet.get_hashtags()            # on affecte a temp (variable temporaire) la liste des hashtags de chaque tweet\n",
    "            for hashtag in temp :                  # on accède à chaque hashtag présent dans temp\n",
    "                l.append(hashtag)                  # on l'affecte à la liste finale qu'on va retourner\n",
    "                \n",
    "    return l\n",
    "    \n",
    "   \n",
    "# La deuxième :\n",
    "def top_hashtag(k):\n",
    "    '''retourne le top k hashtag utilisé dans la database'''\n",
    "    hashtags = all_hashtag()                       # utilisation de la fonction all_hashtag() pour récupérer la liste complète des hashtag\n",
    "    hashtag_count = {}                             # on initialise un dictionnaire vide pour compter chaque occurence de chaque hashtag\n",
    "    for hashtag in hashtags :                      # ici on prend chaque hashtag un par un, ici nommé \"e\" (pour élément)\n",
    "        if hashtag in hashtag_count :              # on vérifie si \"hashtag\" est dans le dictionnaire qui compte les occurences\n",
    "            hashtag_count[hashtag] += 1            # dans ce cas là on incrémente sa valeur de 1\n",
    "        else :\n",
    "            hashtag_count[hashtag] = 1             # dans le cas contraire, on initialise la clé \"hashtag\" à une valeur initiale de 1 \n",
    "\n",
    "    hashtag_count = dict(sorted(hashtag_count.items(),key = lambda x : x[1], reverse=True)) # on tri le dictionnaire en fonction des valeurs avec le lambda x[1] et reverse = True (de manière croissante)\n",
    "    temp = list(hashtag_count.items())[:k]        # on converti le dictionnaire en une liste pour pouvoir effectué un slicing (afin d'obtenir les top k éléments) \n",
    "    top_k_hashtag = dict(temp)                    # on reconverti en dictionnaire afin de pouvoir manipuler les clés et valeurs facilement et dans un odre précis\n",
    "    \n",
    "    # Diagramme avec plt\n",
    "    x_hashtag = list(top_k_hashtag.keys())        # on affecte à la variable x_hashtag les clés du dictionnaire (les hashtags (str))\n",
    "    y_occurence = list(top_k_hashtag.values())    # on affecte à la variable y_occurence les valeurs des clés du dictionnaire (les occurences)\n",
    "\n",
    "    plt.bar(x_hashtag,y_occurence)                # on créer le diagramme en bar avec en x -> x_hashtag et en y -> y_occurence\n",
    "    plt.xlabel(\"Hashtag\")                         # \"Hashtag\" comme titre de l'axe des x\n",
    "    plt.ylabel(\"Occurence\")                       # \"Occurence\" comme titre de l'axe des y\n",
    "\n",
    "    plt.xticks(fontsize=6)                        # on précise la taille du texte pour les valeurs en x (les hashtags)\n",
    "    plt.show()                                    # affichage du diagramme\n",
    "\n",
    "    return top_k_hashtag                          # on retourne en même temps le dictionnaire trié en ordre croissant des top k hashtags\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "# Pour Top K Users -> 2 fonctions \n",
    "\n",
    "# La première : \n",
    "def all_users():\n",
    "    '''retourne tout les utilisateurs présent dans le fichier json dans une liste'''\n",
    "    l = []                                       # on intialise la liste qui va être retournée à l'éxécution de la fonction\n",
    "    for tweet in tweets:                         # on parcours tout les tweets (un par un)\n",
    "        user = tweet.get_author()                # on extrait l'auteur grâce à la méthode .get_author() qu'on place dans la variable user\n",
    "        l.append(user)                           # on ajoute à la liste l'utilisateur/id du tweet\n",
    "                \n",
    "    return l\n",
    "\n",
    "# La deuxième : \n",
    "def top_users(k):\n",
    "    '''retourne le top k d'utilisateurs/id présent dans la database'''\n",
    "    users = all_users()                          # utilisation de la fonction all_users() pour récupérer la liste complète des utilisateurs\n",
    "    user_ntweet = {}                             # on initialise un dictionnaire vide pour compter chaque occurence de chaque id\n",
    "    for user in users :                          # ici on prend chaque id un par un, ici nommé \"user\"\n",
    "        if user in user_ntweet :                 # on vérifie si l'id est déjà dans le dictionnaire\n",
    "            user_ntweet[user] += 1               # si c'est le ce cas, on incrémente sa valeur de 1 \n",
    "        else :\n",
    "            user_ntweet[user] = 1                # sinon on crée une clé pour l'id correspondant et on initie sa valeur à 1 \n",
    "\n",
    "    user_ntweet = dict(sorted(user_ntweet.items(), key = lambda x : x[1], reverse=True)) # on tri le dictionnaire en fonction des valeurs avec le lambda x[1] et reverse = True (de manière croissante)\n",
    "    temp = list(user_ntweet.items())[:k]         # on converti le dictionnaire en une liste pour pouvoir effectué un slicing (afin d'obtenir les top k éléments)\n",
    "    top_k_users = dict(temp)                     # on reconverti en dictionnaire afin de pouvoir manipuler les clés et valeurs facilement et dans un odre précis\n",
    "\n",
    "    x_user = list(top_k_users.keys())            # on affecte à la variable x_user les clés du dictionnaire (les id (str))\n",
    "    y_occurence = list(top_k_users.values())     # on affecte à la variable y_occurence les valeurs du dictionnaire (les occurences)\n",
    "\n",
    "    plt.bar(x_user,y_occurence)                  # on créer le diagramme en bar avec en x -> x_hashtag et en y -> y_occurence\n",
    "    plt.xlabel(\"User\")                           # \"User\" comme titre de l'axe des x\n",
    "    plt.ylabel(\"Occurence\")                      # \"Occurence\" comme titre de l'axe des y\n",
    "\n",
    "    plt.xticks(fontsize=5)                       # on précise la taille du texte pour les valeurs en x (les id)\n",
    "    plt.show()                                   # affichage du diagramme\n",
    "\n",
    "    return top_k_users                           # on retourne en même temps le dictionnaire trié en ordre croissant des top k users\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "# Pour the top K mentions -> 2 fonctions \n",
    " \n",
    "# La première :\n",
    "def all_mentions():\n",
    "    '''retourne toutes les mentions de la database'''\n",
    "    l = []                                        # on initialise la liste qui va être utilisée pour retourner les mentions à l'éxécution de la fonction\n",
    "    for tweet in tweets:\n",
    "        if tweet.get_mention()== [] :             # on vérifie avec l'atribut de l'instance si la valeur retourner est  \"[]\" <- pas de valeur ; dans ce cas là on passe au tweet suivant\n",
    "            continue\n",
    "        else :\n",
    "            mentions = tweet.get_mention()        # on place les mentions obtenues du tweet dans la variable temporaire mentions (qui va correspondre à une liste) \n",
    "            for mention in mentions :             # on parcours chaque élément de la liste (ils peuvent être dans une liste de liste par exemple :[[mention],mention2,...])\n",
    "                l.append(mention)                 # et on l'ajoute à notre liste finale\n",
    "                \n",
    "    return l\n",
    "\n",
    "# La deuxième :\n",
    "def top_mention(k):\n",
    "    '''retourne le top k d'utilisateurs/id présent dans la database'''\n",
    "    mentions = all_mentions()                       # utilisation de la fonction all_mentions() pour récupérer la liste complète des mentions\n",
    "    mention_ntweet = {}                             # on initialise un dictionnaire vide pour compter chaque occurence de chaque mention\n",
    "    for mention in mentions :                       # ici on prend chaque mention, une par une\n",
    "        if mention in mention_ntweet :              # on vérifie si la mention est présente dans le dictionnaire\n",
    "            mention_ntweet[mention] += 1            # dans ce cas là on incrémente la valeur corresponde à la mention (la clé)\n",
    "        else :\n",
    "            mention_ntweet[mention] = 1             # dans le cas contraire on initie la clé correspondante à la mention à 1\n",
    "\n",
    "    mention_ntweet = dict(sorted(mention_ntweet.items(),key = lambda x : x[1], reverse=True)) # on tri le dictionnaire en fonction des valeurs avec le lambda x[1] et reverse = True (de manière croissante)\n",
    "    temp = list(mention_ntweet.items())[:k]         # on converti le dictionnaire en une liste pour pouvoir effectué un slicing (afin d'obtenir les top k éléments)\n",
    "    top_k_mentions = dict(temp)                     # on reconverti la liste en dictionnaire trié en ordre croissant des top k mentions\n",
    "\n",
    "    x_mention = list(top_k_mentions.keys())         # on affecte à la variable x_mention les clés du dictionnaire (les mentions (str))\n",
    "    y_occurence = list(top_k_mentions.values())     # on affecte à la variable y_occurence les valeurs du dictionnaire (les occurences)\n",
    "\n",
    "    plt.bar(x_mention,y_occurence)                  # on créer le diagramme en bar avec en x -> x_mention et en y -> y_occurence\n",
    "    plt.xlabel(\"Mention\",)                          # on créer le diagramme en bar avec en x -> x_mention\n",
    "    plt.ylabel(\"Occurence\")                         # on créer le diagramme en bar avec en y -> y_occurence\n",
    "\n",
    "    plt.xticks(fontsize=7)                          # on précise la taille du texte pour les valeurs en x (les mentions)\n",
    "    plt.show()                                      # affichage du diagramme\n",
    "\n",
    "    return top_k_mentions                           # on retourne en même temps le dictionnaire trié en ordre croissant des top k mentions\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "# Pour the top K topics -> 2 fonctions \n",
    "\n",
    "# La première :\n",
    "def all_topics():\n",
    "    '''retourne tous les topics de la database'''\n",
    "    l = []                                        # on initialise la liste qui va être utilisée pour retourner les mentions à l'éxécution de la fonction\n",
    "    for tweet in tweets:\n",
    "        topics = tweet.get_topic()        # on place les mentions obtenues du tweet dans la variable temporaire mentions (qui va correspondre à une liste) \n",
    "        for topic in topics :             # on parcours chaque élément de la liste (ils peuvent être dans une liste de liste par exemple :[[mention],mention2,...])\n",
    "            l.append(topic)                 # et on l'ajoute à notre liste finale\n",
    "                \n",
    "    return l\n",
    "\n",
    "# La deuxième : \n",
    "def top_topics(k):\n",
    "    '''retourne le top k topics/sujets de tweet présent dans la database'''\n",
    "    topics = all_topics()                               # utilisation de la fonction all_topics() pour récupérer la liste complète des topics\n",
    "    topics_ntweet = {}                                  # on initialise un dictionnaire vide pour compter chaque occurence de chaque topic\n",
    "    for topic in topics :                               # ici on prend chaque topic, un par un\n",
    "        if topic == \"#\" :\n",
    "            continue                              \n",
    "        elif topic in topics_ntweet :                     # on vérifie si le topic est présent dans le dictionnaire\n",
    "            topics_ntweet[topic] += 1                   # dans ce cas là on incrémente la valeur correspondante au topic (la clé)\n",
    "        else :\n",
    "            topics_ntweet[topic] = 1                    # dans le cas contraire on initie la clé correspondante à la mention à 1\n",
    "\n",
    "    topics_ntweet = dict(sorted(topics_ntweet.items(),key = lambda x : x[1], reverse=True)) # on tri le dictionnaire en fonction des valeurs avec le lambda x[1] et reverse = True (de manière croissante)\n",
    "    temp = list(topics_ntweet.items())[:k]              # on converti le dictionnaire en une liste pour pouvoir effectué un slicing (afin d'obtenir les top k éléments)\n",
    "    top_k_topics = dict(temp)                           # on reconverti la liste en dictionnaire trié en ordre croissant des top k topics\n",
    "\n",
    "    x_topic = list(top_k_topics.keys())                 # on affecte à la variable x_topic les clés du dictionnaire (les mentions (str))\n",
    "    y_occurence = list(top_k_topics.values())           # on affecte à la variable y_occurence les valeurs du dictionnaire (les occurences)\n",
    "\n",
    "    plt.bar(x_topic,y_occurence)                        # on créer le diagramme en bar avec en x -> x_topic et en y -> y_occurence\n",
    "    plt.xlabel(\"Topic\",)                                # on créer le diagramme en bar avec en x -> x_topic\n",
    "    plt.ylabel(\"Occurence\")                             # on créer le diagramme en bar avec en y -> y_occurence\n",
    "\n",
    "    plt.xticks(fontsize=7)                              # on précise la taille du texte pour les valeurs en x (les topics)\n",
    "    plt.show()                                          # affichage du diagramme\n",
    "\n",
    "    return top_k_topics \n",
    "    \n",
    "\n",
    "\n",
    "###########################################################################################################################\n",
    "################################################## AVEC LES DATAFRAMES   ##################################################\n",
    "###########################################################################################################################\n",
    "\n",
    "\n",
    "def all_tweet_mention(mention):\n",
    "    '''retourne l'ensemble des tweets mentionnant un utilisateur spécifique dans un dataframe'''                                                               \n",
    "    return df_data[df_data[\"TweetText\"].str.contains(f\"@{mention}\",regex=False)]              # on tri les lignes de la database pour extraire seulement celle qui contiennent @mention dans la colonne \n",
    "\n",
    "def all_tweet_hashtag(hashtag):\n",
    "    '''retourne l'ensemble des tweets faisant référence à un hashtag spécifique'''                                                               \n",
    "    return df_data[df_data[\"TweetText\"].str.contains(f\"#{hashtag}\",regex=False)]              # on tri les lignes de la database pour extraire seulement celle qui contiennent #hashtag dans la colonne de TweetText\n",
    "\n",
    "def user_specified_hashtag(hashtag):\n",
    "    '''retourne les id de chaque utilisateur mentionnant un hashtag spécifique'''                                                         \n",
    "    return df_data[df_data[\"TweetText\"].str.contains(f\"#{hashtag}\",regex=True)].get(\"id\")     # on tri les lignes de la database pour extraire  les #hashtag présent dans la colonne TweetText et on extrait seulement les id\n",
    "\n",
    "def user_mentionned(user):\n",
    "    '''retourne les utilisateurs mentionnés par un utilisateur spécifique'''                                                                \n",
    "    text = str(df_data[df_data[\"id\"]==user].get(\"TweetText\"))                                 # on attribue à la variable text le texte du tweet qui correspond à l'utilisiateur spécifique\n",
    "    return re.findall(r\"@(\\w+)\", text)                                                        # on nettoie le text en extrayant les mentions avec le r\"@(\\w+)\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################# PARTIE SUR LA CREATION DE TWEET #############################################################\n",
    "\n",
    "def creation_time():\n",
    "    '''retourne la date,heure, ... actuelle dans le format utilisé dans le base de donnée'''\n",
    "    actual_time = datetime.now()                                          # on attribue la date actuelle à la variable actual_time \n",
    "    actual_time = actual_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")              # on reformatise la date avec le format correspondant au fichier json, https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior\n",
    "    return actual_time\n",
    "\n",
    "def random_location():\n",
    "    '''retourner une ville aléatoire pour simuler la localisation de l'auteur du tweet'''                                                    \n",
    "    locations =[                                                          # locations est une liste de \"Ville, Pays\"\n",
    "                \"Tokyo, Japan\", \n",
    "                \"New York City, United-States\",\n",
    "                \"London, England\",\n",
    "                \"Pekin, China\",\n",
    "                \"Paris, France\",\n",
    "                \"Montreal, Canada\",\n",
    "                \"Madrid, Spain\",\n",
    "                \"Los Angeles, United-States\",\n",
    "                \"Delhi, India\",\n",
    "                \"Beirut, Lebanon\",\n",
    "                \"Versailles, France\",\n",
    "                \"Amsterdam, Netherland\",\n",
    "                \"Shanghai, China\",\n",
    "                \"Moscou, Russia\",\n",
    "                \"São Paulo, Brasil\"\n",
    "                ]\n",
    "    \n",
    "    return locations[rd.randint(0,len(locations)-1)]                     # on va retourner un ville aléatoire avec l'utilsation du module random (pour l'index)\n",
    "\n",
    "def create_tweet():\n",
    "    '''Fonction qui va permettre d'écrire un tweet et de l'intégrer dans la base de donnée'''  \n",
    "\n",
    "    text_tweet = text_cleaning(input(\"Que voulez vous tweetez ?\"))       # le contenu de notre tweet est récupérer dans la variable text_tweet après être nettoyé\n",
    "    tweet_lang,_ = langid.classify(text_tweet)                           # la langue est extraite de la variable text_tweet grace au module langid\n",
    "    tweet = {                                                            # dictionnaire qui va contenir les informations sur le tweet\n",
    "        \"id\": str(rd.randint(1000000000000000000,2000000000000000000)),  # on attribue un id aléatoire dans un interval semblable à celui de base\n",
    "        \"AuthorLocation\": random_location(),                             # on appelle la fonction random_location() pour générer un emplacement aléatoire pour le tweet\n",
    "        \"CreatedAt\": creation_time(),                                    # on extrait l'heure actuelle où est rédigé le tweet (tout en faisant attention à sa mise en forme, voir fonction)\n",
    "        \"RetweetCount\": rd.randint(1,10),                                # on génère un nombre de retweet\n",
    "        \"TweetLanguage\": tweet_lang,                                     # on extrait la langue du tweet avec le module langid\n",
    "        \"TweetText\": text_tweet                                          \n",
    "            }\n",
    "\n",
    "    try:                                                                 # permet de vérifier si le fichier est non vide\n",
    "        with open(\"fichier_atterrissage.json\", \"r\") as file:\n",
    "            database = json.load(file)                                   # si non vide alors on load/charge le contenu dans la variable database\n",
    "    except FileNotFoundError:                                            # sinon, on crée la base de donnée nous même\n",
    "         \n",
    "         database = []                                                   # création de la base de donnée (en liste pour pouvoir manipuler les tweets correctement)\n",
    "\n",
    "\n",
    "    database.append(tweet)                                               # on ajoute à la suite de la database notre tweet créer\n",
    "\n",
    "    \n",
    "    with open(\"fichier_atterrissage.json\", \"w\") as file:\n",
    "        json.dump(database, file,indent=3,ensure_ascii=False)            # sans le ensure ascii = False on aurait des \\u... quand on mets des accents https://stackoverflow.com/questions/40412714/using-json-dumps-with-ensure-ascii-true\n",
    "\n",
    "create_tweet()\n",
    "\n",
    "file = open(\"fichier_atterrissage.json\",\"r\")\n",
    "data =json.load(file)\n",
    "file.close()\n",
    "\n",
    "# Mise à jour de la database (avec le tweet créé ajouté)\n",
    "data2 = pd.read_json(\"fichier_atterrissage.json\")\n",
    "df_data = pd.DataFrame(data2)\n",
    "#instance/ objets de la class\n",
    "tweets = [Tweet(tweet[\"id\"],tweet[\"AuthorLocation\"],tweet[\"CreatedAt\"],tweet[\"RetweetCount\"],tweet[\"TweetLanguage\"],tweet[\"TweetText\"]) for tweet in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################## TESTS ########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_hashtag(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mention(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_users(5)                                                                    # pas très pertinent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[-1].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[1].get_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hashtag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweet_hashtag(\"InPoDa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[df_data[\"id\"]==1418705513660010496]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mentionned(1418705513660010496)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweet_hashtag(\"MachineLearning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweet_mention(\"Ahmad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mentionned(1415291886860967936)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_hashtag(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for Top k hashtags\n",
    "all_hashtag()\n",
    "top_hashtag(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for Top k users\n",
    "all_users()\n",
    "top_users(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for Top k mentions\n",
    "all_mentions()\n",
    "top_mention(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweet_hashtag(\"DataScience\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweet_mention(\"SpirosMargaris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hashtag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_specified_hashtag(\"FEATURED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>AuthorLocation</th>\n",
       "      <th>CreatedAt</th>\n",
       "      <th>RetweetCount</th>\n",
       "      <th>TweetLanguage</th>\n",
       "      <th>TweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1415291886860967936</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-14T12:47:35Z</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @pacorjo: According to a recent survey, the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id AuthorLocation             CreatedAt  RetweetCount  \\\n",
       "2  1415291886860967936                 2021-07-14T12:47:35Z             1   \n",
       "\n",
       "  TweetLanguage                                          TweetText  \n",
       "2            en  RT @pacorjo: According to a recent survey, the...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data[df_data[\"id\"]==1415291886860967936]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pacorjo']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_mentionned(1415291886860967936)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>AuthorLocation</th>\n",
       "      <th>CreatedAt</th>\n",
       "      <th>RetweetCount</th>\n",
       "      <th>TweetLanguage</th>\n",
       "      <th>TweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1415322365664321536</td>\n",
       "      <td>Help us!</td>\n",
       "      <td>2021-07-14T14:48:42Z</td>\n",
       "      <td>4</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @byLilyV: #FEATURED #COURSES\\n\\nMachine Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1415322360480141312</td>\n",
       "      <td>Bot universe</td>\n",
       "      <td>2021-07-14T14:48:40Z</td>\n",
       "      <td>4</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @byLilyV: #FEATURED #COURSES\\n\\nMachine Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1415322343241617408</td>\n",
       "      <td>Bradford, Yorkshire</td>\n",
       "      <td>2021-07-14T14:48:36Z</td>\n",
       "      <td>4</td>\n",
       "      <td>en</td>\n",
       "      <td>#FEATURED #COURSES\\n\\nMachine Learning, Data S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1415760329229938688</td>\n",
       "      <td>Global</td>\n",
       "      <td>2021-07-15T19:49:00Z</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>Opening Up the Email Marketing Engine to Artif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1415775356389318656</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-15T20:48:43Z</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @MedicaliPhone: General News Article page #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1415911392620781568</td>\n",
       "      <td>Earth</td>\n",
       "      <td>2021-07-16T05:49:17Z</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @byLilyV: #FEATURED #COURSES\\n\\nMachine Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>1416273732809728000</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-17T05:49:05Z</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @GreenTeamToad: $SSFT &amp;gt;&amp;gt; Logiq Levera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>1416364500480036864</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-17T11:49:46Z</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @InstantConnect1: Artificial intelligence i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>1416545758405349376</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-17T23:50:01Z</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>The Future is Artificial Intelligence.  There ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1416666595498160128</td>\n",
       "      <td>Jaipur, India</td>\n",
       "      <td>2021-07-18T07:50:11Z</td>\n",
       "      <td>18</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @byLilyV: #FEATURED #COURSES\\n\\nMachine Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>1416908199442194432</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-18T23:50:14Z</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>CertNexus Certified Artificial Intelligence Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>1417799194581032960</td>\n",
       "      <td>Camp Hill, PA</td>\n",
       "      <td>2021-07-21T10:50:44Z</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>And Now This:  Artificial Intelligence the hot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>1417874814237282304</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-21T15:51:13Z</td>\n",
       "      <td>11</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @byLilyV: #FEATURED #COURSES\\n\\nMachine Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>1417965457349517312</td>\n",
       "      <td>Genève, Suisse</td>\n",
       "      <td>2021-07-21T21:51:24Z</td>\n",
       "      <td>11</td>\n",
       "      <td>fr</td>\n",
       "      <td>RT @akbarth3great: #machinelearning #artificia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>1418161717297565696</td>\n",
       "      <td>Europe</td>\n",
       "      <td>2021-07-22T10:51:16Z</td>\n",
       "      <td>17</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @byLilyV: #FEATURED #COURSES\\n\\nMachine Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>1418373142028951552</td>\n",
       "      <td>Köln, Deutschland</td>\n",
       "      <td>2021-07-23T00:51:23Z</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @techpearce: New artificial intelligence te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>1418403376677593088</td>\n",
       "      <td>India, Karnataka, Bangalore</td>\n",
       "      <td>2021-07-23T02:51:32Z</td>\n",
       "      <td>21</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @byLilyV: #FEATURED #COURSES\\n\\nMachine Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>1418630011741872128</td>\n",
       "      <td>Mysore  and  BERLIN</td>\n",
       "      <td>2021-07-23T17:52:06Z</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @BDAnalyticsnews: Artificial Intelligence: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>1418705503346061312</td>\n",
       "      <td>Cloud Engineer</td>\n",
       "      <td>2021-07-23T22:52:04Z</td>\n",
       "      <td>0</td>\n",
       "      <td>fr</td>\n",
       "      <td>#technologynews #ai #artificialintelligence #m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>1418811157796855808</td>\n",
       "      <td>Genève, Suisse</td>\n",
       "      <td>2021-07-24T05:51:54Z</td>\n",
       "      <td>0</td>\n",
       "      <td>fr</td>\n",
       "      <td>Artificial intelligence #DeepLearning #learnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>1419837878478245888</td>\n",
       "      <td>Bradford, Yorkshire</td>\n",
       "      <td>2021-07-27T01:51:44Z</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>#FEATURED #COURSES\\n\\nMachine Learning, Data S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>1419852992476499968</td>\n",
       "      <td>Dubai</td>\n",
       "      <td>2021-07-27T02:51:47Z</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>RTA uses artificial intelligence, and advanced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>1420397042539450368</td>\n",
       "      <td>Your Network</td>\n",
       "      <td>2021-07-28T14:53:39Z</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @akbarth3great: #artificialintelligence #ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>1420397041348272128</td>\n",
       "      <td>AWS | Azure | GCP | Cloud</td>\n",
       "      <td>2021-07-28T14:53:39Z</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @akbarth3great: #artificialintelligence #ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>1420940046371332096</td>\n",
       "      <td>Global</td>\n",
       "      <td>2021-07-30T02:51:21Z</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>Artificial Intelligence is revolutionizing vet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568</th>\n",
       "      <td>1420940111462731776</td>\n",
       "      <td>Global</td>\n",
       "      <td>2021-07-30T02:51:37Z</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>UW to lead new NSF institute for using artific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>1420955799124389888</td>\n",
       "      <td>Earth</td>\n",
       "      <td>2021-07-30T03:53:57Z</td>\n",
       "      <td>4</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @byLilyV: #FEATURED #COURSES\\n\\nMachine Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>1421227543345045504</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-30T21:53:46Z</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @byLilyV: #FEATURED #COURSES\\n\\nMachine Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>1421378542806446080</td>\n",
       "      <td>Kyiv, Ukraine</td>\n",
       "      <td>2021-07-31T07:53:47Z</td>\n",
       "      <td>5</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @DecisionsSmart: Will Artificial Intelligen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id               AuthorLocation             CreatedAt  \\\n",
       "12    1415322365664321536                     Help us!  2021-07-14T14:48:42Z   \n",
       "13    1415322360480141312                 Bot universe  2021-07-14T14:48:40Z   \n",
       "14    1415322343241617408          Bradford, Yorkshire  2021-07-14T14:48:36Z   \n",
       "147   1415760329229938688                       Global  2021-07-15T19:49:00Z   \n",
       "155   1415775356389318656                               2021-07-15T20:48:43Z   \n",
       "196   1415911392620781568                       Earth   2021-07-16T05:49:17Z   \n",
       "300   1416273732809728000                               2021-07-17T05:49:05Z   \n",
       "330   1416364500480036864                               2021-07-17T11:49:46Z   \n",
       "388   1416545758405349376                               2021-07-17T23:50:01Z   \n",
       "417   1416666595498160128                Jaipur, India  2021-07-18T07:50:11Z   \n",
       "483   1416908199442194432                               2021-07-18T23:50:14Z   \n",
       "712   1417799194581032960                Camp Hill, PA  2021-07-21T10:50:44Z   \n",
       "736   1417874814237282304                               2021-07-21T15:51:13Z   \n",
       "764   1417965457349517312               Genève, Suisse  2021-07-21T21:51:24Z   \n",
       "812   1418161717297565696                       Europe  2021-07-22T10:51:16Z   \n",
       "875   1418373142028951552            Köln, Deutschland  2021-07-23T00:51:23Z   \n",
       "882   1418403376677593088  India, Karnataka, Bangalore  2021-07-23T02:51:32Z   \n",
       "950   1418630011741872128          Mysore  and  BERLIN  2021-07-23T17:52:06Z   \n",
       "971   1418705503346061312               Cloud Engineer  2021-07-23T22:52:04Z   \n",
       "1001  1418811157796855808               Genève, Suisse  2021-07-24T05:51:54Z   \n",
       "1266  1419837878478245888          Bradford, Yorkshire  2021-07-27T01:51:44Z   \n",
       "1268  1419852992476499968                        Dubai  2021-07-27T02:51:47Z   \n",
       "1396  1420397042539450368                 Your Network  2021-07-28T14:53:39Z   \n",
       "1399  1420397041348272128    AWS | Azure | GCP | Cloud  2021-07-28T14:53:39Z   \n",
       "1566  1420940046371332096                       Global  2021-07-30T02:51:21Z   \n",
       "1568  1420940111462731776                       Global  2021-07-30T02:51:37Z   \n",
       "1570  1420955799124389888                       Earth   2021-07-30T03:53:57Z   \n",
       "1650  1421227543345045504                               2021-07-30T21:53:46Z   \n",
       "1691  1421378542806446080                Kyiv, Ukraine  2021-07-31T07:53:47Z   \n",
       "\n",
       "      RetweetCount TweetLanguage  \\\n",
       "12               4            en   \n",
       "13               4            en   \n",
       "14               4            en   \n",
       "147              0            en   \n",
       "155              2            en   \n",
       "196              6            en   \n",
       "300              3            en   \n",
       "330              2            en   \n",
       "388              0            en   \n",
       "417             18            en   \n",
       "483              0            en   \n",
       "712              0            en   \n",
       "736             11            en   \n",
       "764             11            fr   \n",
       "812             17            en   \n",
       "875              2            en   \n",
       "882             21            en   \n",
       "950              1            en   \n",
       "971              0            fr   \n",
       "1001             0            fr   \n",
       "1266             1            en   \n",
       "1268             0            en   \n",
       "1396             3            en   \n",
       "1399             3            en   \n",
       "1566             0            en   \n",
       "1568             0            en   \n",
       "1570             4            en   \n",
       "1650             6            en   \n",
       "1691             5            en   \n",
       "\n",
       "                                              TweetText  \n",
       "12    RT @byLilyV: #FEATURED #COURSES\\n\\nMachine Lea...  \n",
       "13    RT @byLilyV: #FEATURED #COURSES\\n\\nMachine Lea...  \n",
       "14    #FEATURED #COURSES\\n\\nMachine Learning, Data S...  \n",
       "147   Opening Up the Email Marketing Engine to Artif...  \n",
       "155   RT @MedicaliPhone: General News Article page #...  \n",
       "196   RT @byLilyV: #FEATURED #COURSES\\n\\nMachine Lea...  \n",
       "300   RT @GreenTeamToad: $SSFT &gt;&gt; Logiq Levera...  \n",
       "330   RT @InstantConnect1: Artificial intelligence i...  \n",
       "388   The Future is Artificial Intelligence.  There ...  \n",
       "417   RT @byLilyV: #FEATURED #COURSES\\n\\nMachine Lea...  \n",
       "483   CertNexus Certified Artificial Intelligence Pr...  \n",
       "712   And Now This:  Artificial Intelligence the hot...  \n",
       "736   RT @byLilyV: #FEATURED #COURSES\\n\\nMachine Lea...  \n",
       "764   RT @akbarth3great: #machinelearning #artificia...  \n",
       "812   RT @byLilyV: #FEATURED #COURSES\\n\\nMachine Lea...  \n",
       "875   RT @techpearce: New artificial intelligence te...  \n",
       "882   RT @byLilyV: #FEATURED #COURSES\\n\\nMachine Lea...  \n",
       "950   RT @BDAnalyticsnews: Artificial Intelligence: ...  \n",
       "971   #technologynews #ai #artificialintelligence #m...  \n",
       "1001  Artificial intelligence #DeepLearning #learnin...  \n",
       "1266  #FEATURED #COURSES\\n\\nMachine Learning, Data S...  \n",
       "1268  RTA uses artificial intelligence, and advanced...  \n",
       "1396  RT @akbarth3great: #artificialintelligence #ma...  \n",
       "1399  RT @akbarth3great: #artificialintelligence #ma...  \n",
       "1566  Artificial Intelligence is revolutionizing vet...  \n",
       "1568  UW to lead new NSF institute for using artific...  \n",
       "1570  RT @byLilyV: #FEATURED #COURSES\\n\\nMachine Lea...  \n",
       "1650  RT @byLilyV: #FEATURED #COURSES\\n\\nMachine Lea...  \n",
       "1691  RT @DecisionsSmart: Will Artificial Intelligen...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweet_hashtag(\"machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>AuthorLocation</th>\n",
       "      <th>CreatedAt</th>\n",
       "      <th>RetweetCount</th>\n",
       "      <th>TweetLanguage</th>\n",
       "      <th>TweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1415291947560828928</td>\n",
       "      <td>Mysore  and  BERLIN</td>\n",
       "      <td>2021-07-14T12:47:49Z</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @HDataSystems: Artificial Intelligence and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1415291877897605120</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-14T12:47:33Z</td>\n",
       "      <td>246</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @adgpi: Army Technology Board conducted the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1415291886860967936</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-14T12:47:35Z</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @pacorjo: According to a recent survey, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1415291968700264448</td>\n",
       "      <td>Internet</td>\n",
       "      <td>2021-07-14T12:47:54Z</td>\n",
       "      <td>20</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @HarbRimah: Making AI Sing https://t.co/FJo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1415292139941109760</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-14T12:48:35Z</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @weblineglobal: The applications of #artifi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id       AuthorLocation             CreatedAt  \\\n",
       "0  1415291947560828928  Mysore  and  BERLIN  2021-07-14T12:47:49Z   \n",
       "1  1415291877897605120                       2021-07-14T12:47:33Z   \n",
       "2  1415291886860967936                       2021-07-14T12:47:35Z   \n",
       "3  1415291968700264448             Internet  2021-07-14T12:47:54Z   \n",
       "4  1415292139941109760                       2021-07-14T12:48:35Z   \n",
       "\n",
       "   RetweetCount TweetLanguage  \\\n",
       "0             2            en   \n",
       "1           246            en   \n",
       "2             1            en   \n",
       "3            20            en   \n",
       "4             1            en   \n",
       "\n",
       "                                           TweetText  \n",
       "0  RT @HDataSystems: Artificial Intelligence and ...  \n",
       "1  RT @adgpi: Army Technology Board conducted the...  \n",
       "2  RT @pacorjo: According to a recent survey, the...  \n",
       "3  RT @HarbRimah: Making AI Sing https://t.co/FJo...  \n",
       "4  RT @weblineglobal: The applications of #artifi...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1415291947560828933'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0].get_author()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'En été il fait très chaud , par contre en hiver il fait super froid '"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaning(\"En été il fait très chaud 🌞, par contre en hiver il fait super froid 🥶\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
